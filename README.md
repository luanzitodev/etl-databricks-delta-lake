#  Pipeline de ETL com PySpark e Arquitetura MedalhÃ£o

Este projeto de portfÃ³lio demonstra a criaÃ§Ã£o de um pipeline bÃ¡sico de Engenharia de Dados, seguindo a famosa Arquitetura MedalhÃ£o (Bronze, Silver e Gold).  
O objetivo Ã© transformar dados brutos de vendas em informaÃ§Ãµes prontas para anÃ¡lise, utilizando PySpark e Delta Lake para garantir a qualidade e a organizaÃ§Ã£o em um Data Lake.

---

##  Sobre o Projeto (O que foi feito)

Desenvolvi um pipeline de ETL simples, mas que mostra a organizaÃ§Ã£o de um bom Data Lake.  
**Fonte de Dados:** Um arquivo CSV de vendas (`data/vendas.csv`).  
**Ferramentas:** PySpark para processamento, Delta Lake para armazenamento confiÃ¡vel e Databricks (ambiente de desenvolvimento) como plataforma.  
**Processo:** O projeto realiza limpeza, normalizaÃ§Ã£o e agregaÃ§Ãµes para gerar mÃ©tricas de negÃ³cio.

**Habilidades em Destaque:**
- **Arquitetura MedalhÃ£o ğŸ¥‰ğŸ¥ˆğŸ¥‡:** ImplementaÃ§Ã£o das 3 camadas para refinamento e governanÃ§a dos dados.  
- **PySpark/DataFrames:** Uso do Spark para processamento distribuÃ­do.  
- **TransformaÃ§Ãµes ETL:** AplicaÃ§Ã£o de lÃ³gica para garantia de schema, conversÃ£o de tipos (ex: datas) e criaÃ§Ã£o de novas colunas calculadas.  
- **AgregaÃ§Ã£o de Dados:** GeraÃ§Ã£o de mÃ©tricas (Receita Total DiÃ¡ria) na camada Gold, otimizadas para consumo BI/AnalÃ­tico.

---

##  Estrutura do RepositÃ³rio

O projeto estÃ¡ organizado com foco em clareza e separaÃ§Ã£o de responsabilidades:

```
etl-databricks-delta-lake/
â”œâ”€â”€ data/                    # ContÃ©m o arquivo CSV de origem.
â”œâ”€â”€ images/                  # ContÃ©m as capturas de tela usadas na documentaÃ§Ã£o.
â”œâ”€â”€ notebooks/               # Scripts PySpark do pipeline (Bronze, Silver, Gold).
â”œâ”€â”€ .gitignore               # Ignora arquivos de ambiente e cache.
â””â”€â”€ requirements.txt         # DependÃªncias necessÃ¡rias para execuÃ§Ã£o.
```

---

##  O Fluxo de Refinamento (Camada por Camada)

### 1. Camada Bronze (IngestÃ£o/Raw) ğŸ¥‰
**FunÃ§Ã£o:** Ingerir o CSV e salvÃ¡-lo como uma Delta Table.  
**TransformaÃ§Ã£o:** MÃ­nima. Apenas a garantia do schema inicial, como a conversÃ£o da coluna `order_date` para o tipo `date`.  
ğŸ“¸ <img width="1455" height="1169" alt="ingestao_bronze" src="https://github.com/user-attachments/assets/eae09e06-c1fe-40cb-8304-49cd59d1909a" />


---

### 2. Camada Silver (Refinamento/Limpeza) ğŸ¥ˆ
**FunÃ§Ã£o:** Limpar e enriquecer os dados.  
**TransformaÃ§Ã£o Principal:** CriaÃ§Ã£o da coluna `total_revenue` (preÃ§o Ã— quantidade). O dado agora estÃ¡ mais limpo e pronto para a modelagem.  
ğŸ“¸ <img width="1474" height="1186" alt="transformacao_prata" src="https://github.com/user-attachments/assets/793b3e26-d3f7-4da6-8c7a-f60cde55a907" />



---

### 3. Camada Gold (Curadoria/Consumo) ğŸ¥‡
**FunÃ§Ã£o:** Modelar e agregar o dado para o consumo final.  
**TransformaÃ§Ã£o Principal:** CÃ¡lculo da Receita Total DiÃ¡ria (`total_revenue_daily`). Essa tabela Ã© otimizada para ser consultada por ferramentas de Business Intelligence (BI).  
ğŸ“¸ <img width="1464" height="1182" alt="agregacao_ouro" src="https://github.com/user-attachments/assets/ba91f241-76bf-47eb-8e78-c8f01131b126" />


---

##  ExecuÃ§Ã£o (Foco em Databricks)

Este projeto foi desenvolvido e testado no ambiente **Databricks Community Edition**.  

Para rodar, o processo mais simples Ã©:

1. **Importar:** Clone este repositÃ³rio diretamente para um Databricks Workspace usando a integraÃ§Ã£o Git.  
2. **Configurar:** Anexar os notebooks a um Cluster Spark.  
3. **Executar Sequencialmente:** Os scripts em `notebooks/` devem ser executados em ordem para garantir que as tabelas sejam criadas corretamente:

   ```
   ingestao_bronze.py
   transformacao_prata.py
   agregacao_ouro.py
   ```
